{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment 3: RL Gym\n",
    "### Game Selection: FrozenLake\n",
    "For this assignment I have chosen the simple game Frozen Lake due to its straighforward mechanics and clear reward structure. The AI is rewarded when it reaches the end of the maze without falling into a hole. As this game has a discrete observation space instead of a continous one, the algorithm used can be much simpler. https://gymnasium.farama.org/environments/toy_text/frozen_lake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#Pre-setup installs\n",
    "%pip install gymnasium[ToyText]\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup/imports\n",
    "import gymnasium\n",
    "from gymnasium import wrappers\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "env = gymnasium.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")  # create the environment used for the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to generate video recordings every 1000 runs or so\n",
    "env = wrappers.RecordVideo(env, 'recordings')  # wrap environment in recorder to view output\n",
    "if not os.path.exists('recordings'):  # create directory for storing videos\n",
    "    os.makedirs('recordings')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation: \n",
    "For this game, I chose to use the Q-learning algorithm, primarily as it is one of the simplest algorithms that can be used to show learning and improvement. I modified this by adding an exploration rate that decays over time, meaning the model will rely more and more on its learned behaviours. \n",
    "The hyperparameters for this algorithm, shown below, were chosen based on trial and error. I found a higher learning rate would overfit quickly and do worse as the run count increased. With the hyperparameters shown below, the training code can reliably generate a model which can solve the Frozen Lake puzzle (\"solving\" meaning have a best 100-run average of at least 0.78) in about 6000 runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "number_of_runs = 10000  # takes less than 3 seconds when not recording\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "initial_exploration = 1.0\n",
    "min_exploration = 0.01\n",
    "exploration_decay = 0.001\n",
    "report_interval = 500\n",
    "report = 'Average: %.2f, 100-run average: %.2f, Best average: %.2f (Run %d)'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process: \n",
    "To train the model, I used a table of q-values to track the best move for each step, and a decaying exploration rate which would allow the AI to take random actions and learn their outcomes, which it would do less and less over the course of many runs. In terms of pre-processing, the gymnasium environment provides the map to the AI not as an image, but as an array of letters which represent terrain features. This makes it much easier for the AI to process many runs in a short period of time (10,000 runs took less than 3 seconds on my machine) so training can be done quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset learned values, rewards and best streak\n",
    "q_table = np.zeros((env.observation_space.n, env.action_space.n)) # stores learned values\n",
    "rewards = []\n",
    "best_streak = 0.0\n",
    "\n",
    "# Start training\n",
    "for run in range(number_of_runs):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    run_reward = 0\n",
    "    exploration_rate = max(min_exploration, initial_exploration * np.exp(-exploration_decay * run)) # decrease exploration rate every run\n",
    "    while not done:\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            action = env.action_space.sample()  # Take random actions\n",
    "        else:\n",
    "            action = np.argmax(q_table[observation, :])  # Take learned action \n",
    "\n",
    "        new_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        q_table[observation, action] = (1 - learning_rate) * q_table[observation, action] + learning_rate * \\\n",
    "            (reward + discount_factor * np.max(q_table[new_observation, :]))\n",
    "        \n",
    "        run_reward += reward        \n",
    "        observation = new_observation\n",
    "        \n",
    "        if (run + 1) % 100 == 0: # check if last 100 run average was the best so far\n",
    "            current_streak = np.mean(rewards[-100:])\n",
    "            if current_streak > best_streak:\n",
    "                best_streak = current_streak\n",
    "\n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "            rewards.append(run_reward)\n",
    "            if ((run + 1) % report_interval == 0): # every 500 runs, print a report showing progress\n",
    "                print(report % (np.mean(rewards), np.mean(rewards[-100:]), best_streak, run + 1))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Evidence:\n",
    "#### Before training:\n",
    "<img src=\"rl-video-episode-27.gif\" />\n",
    "\n",
    "\n",
    "When starting out, the AI tends to wander around randomly, and often falls in a hole very quickly.\n",
    "\n",
    "\n",
    "#### After 1000 runs:\n",
    "<img src=\"rl-video-episode-1000.gif\" />\n",
    "\n",
    "\n",
    "After around 1000 runs, the AI can usually reach the goal about 30% of the time.\n",
    "\n",
    "\n",
    "#### After 5000 runs:\n",
    "<img src=\"rl-video-episode-5000.gif\" />\n",
    "\n",
    "\n",
    "After 5000 runs, the AI still occasionally falls in holes while exploring, but much less often.\n",
    "\n",
    "\n",
    "#### After 10000 runs:\n",
    "<img src=\"rl-video-episode-9000.gif\" />\n",
    "\n",
    "\n",
    "After 6000-8000 runs, the AI tends to get to the goal in more than 80% of runs, though it doesn't usually take the most direct path as there is no incentive for getting to the goal quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation and Report:\n",
    "The process of picking a game, picking an algorithm, training the model, refining the model, and then exporting the results took enormous effort, and there were hurdles at every step.\n",
    "\n",
    "\n",
    "Initially I had planned to create an AI for CartPole, but as CartPole uses a continuous observation space rather than a discrete one like Frozen Lake, this would have limited the types of algorithms I could use to more complex ones that were much harder to configure. I ended up changing to Frozen Lake to get around this limitation.\n",
    "\n",
    "\n",
    "Picking the algorithm involved a fair amount of research, and I chose the Q-learning algorithm as the other options (DQN, DDPG) were too complex for me to confidently write myself, especially with the timeframe given to complete this project, and the current lack of up-to-date examples for using the Gymnasium environment. \n",
    "\n",
    "\n",
    "Training the model required me to search the Gymnasium API to find out what kind of results were being returned from its various methods, to understand why some inputs were resulting in no learning happening at all. Ultimately familiarising myself with the API made it much easier to both design the model and export the results when I was done.\n",
    "\n",
    "\n",
    "Refining the model was mostly trial and error, and some tweaks resulted in significantly worse performance, like setting the learning rate too high. I also found that setting the discount rate too low resulted in much slower improvement, and a tendency to stall around a 40% success rate. Once I changed the exploration rate from a fixed value to scale down over time, I found that learning happenend much faster and had significantly better results (solving the game in 5000 runs rather than 50,000). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

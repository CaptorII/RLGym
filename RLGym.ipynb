{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment 3: RL Gym\n",
    "### Game Selection: FrozenLake\n",
    "For this assignment I have chosen the simple game Frozen Lake due to its straighforward mechanics and clear reward structure. The AI is rewarded when it reaches the end of the maze without falling into a hole. As this game has a discrete observation space instead of a continous one, the algorithms used can be much simpler. https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    "\n",
    "Using some code from: \n",
    "- https://www.sliceofexperiments.com/p/an-actually-runnable-march-2023-tutorial \n",
    "- https://medium.com/analytics-vidhya/q-learning-is-the-most-basic-form-of-reinforcement-learning-which-doesnt-take-advantage-of-any-8944e02570c5\n",
    "- https://gist.github.com/maciejbalawejder/d028e0ddc4c88c19d3761e58fb90c137#file-q-learning-py\n",
    "- https://www.baeldung.com/cs/epsilon-greedy-q-learning\n",
    "- https://www.digitalocean.com/community/tutorials/how-to-build-atari-bot-with-openai-gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#Pre-setup installs\n",
    "%pip install gymnasium[ToyText]\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup/imports\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "\n",
    "env = gymnasium.make(\"FrozenLake-v1\")  # create the environment used for the game"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation: \n",
    "For this game, I chose to use the Q-learning algorithm, primarily as it is one of the simplest algorithms that can be used to show learning and improvement. I modified this by adding an exploration rate that decays over time, meaning the model will rely more and more on its learned behaviours. \n",
    "The hyperparameters for this algorithm, shown below, were chosen based on trial and error. I found a higher learning rate would overfit quickly and do worse as the run count increased. With the hyperparameters shown below, the training code can reliably generate a model which can solve the Frozen Lake puzzle (\"solving\" meaning have a best 100-run average of at least 0.78) in about 6000 runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "number_of_runs = 10000  # takes about 3 seconds\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "initial_exploration = 1.0\n",
    "min_exploration = 0.01\n",
    "exploration_decay = 0.001\n",
    "report_interval = 500\n",
    "report = 'Average: %.2f, 100-run average: %.2f, Best average: %.2f (Run %d)'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process: \n",
    "Describe the training process, including any pre-processing steps such as frame stacking or converting frames to grayscale. Take short (<10 sec) videos at suitable training steps to demonstrate the agent's progress. Provide commentary on the agent's performance and any notable observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset learned values, rewards and best streak\n",
    "q_table = np.zeros((env.observation_space.n, env.action_space.n)) # stores learned values\n",
    "rewards = []\n",
    "best_streak = 0.0\n",
    "\n",
    "# Start training\n",
    "for run in range(number_of_runs):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    run_reward = 0\n",
    "    exploration_rate = max(min_exploration, initial_exploration * np.exp(-exploration_decay * run)) # decrease exploration rate every run\n",
    "    while not done:\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            action = env.action_space.sample()  # Take random actions\n",
    "        else:\n",
    "            action = np.argmax(q_table[observation, :])  # Take learned action \n",
    "\n",
    "        new_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        q_table[observation, action] = (1 - learning_rate) * q_table[observation, action] + learning_rate * \\\n",
    "            (reward + discount_factor * np.max(q_table[new_observation, :]))\n",
    "        \n",
    "        run_reward += reward        \n",
    "        observation = new_observation\n",
    "        \n",
    "        if (run + 1) % 100 == 0: # check if last 100 run average was the best so far\n",
    "            current_streak = np.mean(rewards[-100:])\n",
    "            if current_streak > best_streak:\n",
    "                best_streak = current_streak\n",
    "\n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "            rewards.append(run_reward)\n",
    "            if ((run + 1) % report_interval == 0): # every 500 runs, print a report showing progress\n",
    "                print(report % (np.mean(rewards), np.mean(rewards[-100:]), best_streak, run + 1))\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Performance Metrics: \n",
    "Evaluate the performance of your trained model. Provide relevant metrics such as average reward, episodes needed to solve the game, and any additional visualizations or graphs. Comment on the strengths and limitations of your trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation and Report: \n",
    "Provide a clear and detailed report of your process, including decisions, challenges, and any improvements made during the training. Include commentary on the weights chosen and any pre-processing techniques applied."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

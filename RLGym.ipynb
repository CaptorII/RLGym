{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment 3: RL Gym\n",
    "### Game Selection: FrozenLake\n",
    "For this assignment I have chosen the simple game Frozen Lake due to its straighforward mechanics and clear reward structure. The AI is rewarded when it reaches the end of the maze without falling into a hole. As this game has a discrete observation space instead of a continous one, the algorithm used can be much simpler. https://gymnasium.farama.org/environments/toy_text/frozen_lake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#Pre-setup installs\n",
    "%pip install gymnasium[ToyText]\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup/imports\n",
    "import gymnasium\n",
    "from gymnasium import wrappers\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "wrapped_env = gymnasium.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")  # create the environment used for the game\n",
    "env = wrappers.RecordVideo(wrapped_env, 'game_screenshots')  # wrap environment in recorder to view output\n",
    "if not os.path.exists('game_screenshots'):  # create directory for storing videos\n",
    "    os.makedirs('game_screenshots')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation: \n",
    "For this game, I chose to use the Q-learning algorithm, primarily as it is one of the simplest algorithms that can be used to show learning and improvement. I modified this by adding an exploration rate that decays over time, meaning the model will rely more and more on its learned behaviours. \n",
    "The hyperparameters for this algorithm, shown below, were chosen based on trial and error. I found a higher learning rate would overfit quickly and do worse as the run count increased. With the hyperparameters shown below, the training code can reliably generate a model which can solve the Frozen Lake puzzle (\"solving\" meaning have a best 100-run average of at least 0.78) in about 6000 runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "number_of_runs = 10000  # takes about 3 seconds\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "initial_exploration = 1.0\n",
    "min_exploration = 0.01\n",
    "exploration_decay = 0.001\n",
    "report_interval = 500\n",
    "report = 'Average: %.2f, 100-run average: %.2f, Best average: %.2f (Run %d)'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process: \n",
    "Describe the training process, including any pre-processing steps such as frame stacking or converting frames to grayscale. Take short (<10 sec) videos at suitable training steps to demonstrate the agent's progress. Provide commentary on the agent's performance and any notable observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset learned values, rewards and best streak\n",
    "q_table = np.zeros((env.observation_space.n, env.action_space.n)) # stores learned values\n",
    "rewards = []\n",
    "best_streak = 0.0\n",
    "\n",
    "# Start training\n",
    "for run in range(number_of_runs):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    run_reward = 0\n",
    "    exploration_rate = max(min_exploration, initial_exploration * np.exp(-exploration_decay * run)) # decrease exploration rate every run\n",
    "    while not done:\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            action = env.action_space.sample()  # Take random actions\n",
    "        else:\n",
    "            action = np.argmax(q_table[observation, :])  # Take learned action \n",
    "\n",
    "        new_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        q_table[observation, action] = (1 - learning_rate) * q_table[observation, action] + learning_rate * \\\n",
    "            (reward + discount_factor * np.max(q_table[new_observation, :]))\n",
    "        \n",
    "        run_reward += reward        \n",
    "        observation = new_observation\n",
    "        \n",
    "        if (run + 1) % 100 == 0: # check if last 100 run average was the best so far\n",
    "            current_streak = np.mean(rewards[-100:])\n",
    "            if current_streak > best_streak:\n",
    "                best_streak = current_streak\n",
    "\n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "            rewards.append(run_reward)\n",
    "            if ((run + 1) % report_interval == 0): # every 500 runs, print a report showing progress\n",
    "                print(report % (np.mean(rewards), np.mean(rewards[-100:]), best_streak, run + 1))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training progress:\n",
    "#### Before training:\n",
    "<img src=\"rl-video-episode-27.gif\" />\n",
    "\n",
    "\n",
    "When starting out, the AI tends to wander around randomly, and often falls in a hole very quickly.\n",
    "\n",
    "\n",
    "#### After 1000 runs:\n",
    "<img src=\"rl-video-episode-1000.gif\" />\n",
    "\n",
    "\n",
    "After around 1000 runs, the AI can usually reach the goal about 30% of the time.\n",
    "\n",
    "\n",
    "#### After 5000 runs:\n",
    "<img src=\"rl-video-episode-5000.gif\" />\n",
    "\n",
    "\n",
    "After 5000 runs, the AI still occasionally falls in holes while exploring.\n",
    "\n",
    "\n",
    "#### After 9000 runs:\n",
    "<img src=\"rl-video-episode-9000.gif\" />\n",
    "\n",
    "\n",
    "After 6000-8000 runs, the AI tends to get to the goal most of the time, though it doesn't usually take the most direct path as there is no incentive for getting to the goal quickly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Performance Metrics: \n",
    "Evaluate the performance of your trained model. Provide relevant metrics such as average reward, episodes needed to solve the game, and any additional visualizations or graphs. Comment on the strengths and limitations of your trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation and Report: \n",
    "Provide a clear and detailed report of your process, including decisions, challenges, and any improvements made during the training. Include commentary on the weights chosen and any pre-processing techniques applied."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

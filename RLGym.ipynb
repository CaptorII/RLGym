{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment 3: RL Gym\n",
    "### Game Selection: CartPole\n",
    "For this assignment I have chosen the 2D training tool CartPole due to its straighforward mechanics and clear reward structure. The AI is rewarded every time it takes a step and is still alive, so can be trained to improve the time it keeps the pole upright and thus stays alive for. https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "\n",
    "\n",
    "Using some code from: \n",
    "- https://www.sliceofexperiments.com/p/an-actually-runnable-march-2023-tutorial \n",
    "- https://medium.com/analytics-vidhya/q-learning-is-the-most-basic-form-of-reinforcement-learning-which-doesnt-take-advantage-of-any-8944e02570c5\n",
    "- https://gist.github.com/maciejbalawejder/d028e0ddc4c88c19d3761e58fb90c137#file-q-learning-py\n",
    "- https://www.baeldung.com/cs/epsilon-greedy-q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#Pre-setup installs\n",
    "%pip install gymnasium[classic-control]\n",
    "%pip install gymnasium[box2d]\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup/imports\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "\n",
    "env = gymnasium.make(\"CartPole-v1\")  # create the environment used for the game"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation: \n",
    "Implement and train an RL model using an algorithm like Q-learning, Deep Q-Networks (DQN), or any other suitable method. Explain your choice of algorithm and any modifications you made. Comment on the hyperparameters and why you chose them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "number_of_runs = 10000  # takes about 1.5 seconds\n",
    "learning_rate = 0.15\n",
    "discount_factor = 0.99\n",
    "exploration = 0.1\n",
    "q_table = np.zeros((env.observation_space.shape[0], env.action_space.n)) # stores learned values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process: \n",
    "Describe the training process, including any pre-processing steps such as frame stacking or converting frames to grayscale. Take short (<10 sec) videos at suitable training steps to demonstrate the agent's progress. Provide commentary on the agent's performance and any notable observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "observations, actions = env.observation_space, env.action_space\n",
    "\n",
    "for run in range(number_of_runs):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.uniform(0,1) < exploration:\n",
    "            action = actions.sample()  # Take random actions\n",
    "        else:\n",
    "            action = np.argmax(q_table[observation, :])  # Take learned action \n",
    "\n",
    "        new_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        q_table[observation, action] = q_table[observation, action] + learning_rate * \\\n",
    "            (reward + discount_factor * np.max(q_table[new_observation, :]) - q_table[observation, action])\n",
    "        \n",
    "        observation = new_observation\n",
    "\n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "            if (reward > 1.0):\n",
    "                print(reward + \" on run \" + run)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Performance Metrics: \n",
    "Evaluate the performance of your trained model. Provide relevant metrics such as average reward, episodes needed to solve the game, and any additional visualizations or graphs. Comment on the strengths and limitations of your trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "env = gymnasium.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation and Report: \n",
    "Provide a clear and detailed report of your process, including decisions, challenges, and any improvements made during the training. Include commentary on the weights chosen and any pre-processing techniques applied."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

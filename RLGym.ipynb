{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment 3: RL Gym\n",
    "### Game Selection: Breakout for Atari\n",
    "For this assignment I have chosen the Atari game Breakout, as it is simple enough that it should be relatively easy to get a demonstrable model working without an excessive amount of training. https://gymnasium.farama.org/environments/atari/breakout/\n",
    "#### SCORING\n",
    "The player scores points by hitting one of the wall's bricks. The number of points is determined by the brick's color:\n",
    "- Red - 7 points\n",
    "- Orange - 7 points\n",
    "- Yellow - 4 points\n",
    "- Green - 4 points\n",
    "- Aqua - 1 point\n",
    "- Blue - 1 point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#Pre-setup installs\n",
    "%pip install gymnasium[atari]\n",
    "%pip install gymnasium[accept-rom-license]\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup/imports\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import gymnasium\n",
    "\n",
    "env = gymnasium.make(\"ALE/Breakout-v5\", obs_type=\"rgb\") # create the environment used for the game"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation: \n",
    "Implement and train an RL model using an algorithm like Q-learning, Deep Q-Networks (DQN), or any other suitable method. Explain your choice of algorithm and any modifications you made. Comment on the hyperparameters and why you chose them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agent for playing Breakout\n",
    "class BreakoutAgent:\n",
    "    def __init__(self, learning_rate, discount_factor, exploration):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration = exploration\n",
    "        self.rewards = []\n",
    "\n",
    "    def take_action(self, observation):\n",
    "\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent and define hyperparameters\n",
    "number_of_runs = 1000\n",
    "learning_rate = 0.15\n",
    "discount_factor = 0.99\n",
    "exploration = lambda run: 50. / (run + 10)\n",
    "\n",
    "agent = BreakoutAgent(learning_rate, discount_factor, exploration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process: \n",
    "Describe the training process, including any pre-processing steps such as frame stacking or converting frames to grayscale. Take short (<10 sec) videos at suitable training steps to demonstrate the agent's progress. Provide commentary on the agent's performance and any notable observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "observations, actions = env.observation_space, env.action_space\n",
    "\n",
    "for run in range(1, number_of_runs + 1):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.random() < agent.exploration:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = agent.take_action() # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - agent.learning_rate) * old_value + agent.learning_rate * (reward + next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        action = agent.take_action()\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "            observation, info = env.reset()\n",
    "            agent.rewards.append(reward)\n",
    "            #if run % (number_of_runs / 10) == 0:\n",
    "                #print(agent.rewards)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Performance Metrics: \n",
    "Evaluate the performance of your trained model. Provide relevant metrics such as average reward, episodes needed to solve the game, and any additional visualizations or graphs. Comment on the strengths and limitations of your trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation and Report: \n",
    "Provide a clear and detailed report of your process, including decisions, challenges, and any improvements made during the training. Include commentary on the weights chosen and any pre-processing techniques applied."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

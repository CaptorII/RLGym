{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment 3: RL Gym\n",
    "### Game Selection: CartPole\n",
    "For this assignment I have chosen the 2D training tool CartPole due to its straighforward mechanics and clear reward structure. The AI is rewarded every time it takes a step and is still alive, so can be trained to improve the time it keeps the pole upright and thus stays alive for. https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "\n",
    "\n",
    "Using some code from: \n",
    "- https://www.sliceofexperiments.com/p/an-actually-runnable-march-2023-tutorial \n",
    "- https://medium.com/analytics-vidhya/q-learning-is-the-most-basic-form-of-reinforcement-learning-which-doesnt-take-advantage-of-any-8944e02570c5\n",
    "- https://gist.github.com/maciejbalawejder/d028e0ddc4c88c19d3761e58fb90c137#file-q-learning-py\n",
    "- https://www.baeldung.com/cs/epsilon-greedy-q-learning\n",
    "- https://www.digitalocean.com/community/tutorials/how-to-build-atari-bot-with-openai-gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#Pre-setup installs\n",
    "%pip install gymnasium[classic-control]\n",
    "%pip install gymnasium[ToyText]\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup/imports\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "\n",
    "env = gymnasium.make(\"FrozenLake-v1\")  # create the environment used for the game"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation: \n",
    "Implement and train an RL model using an algorithm like Q-learning, Deep Q-Networks (DQN), or any other suitable method. Explain your choice of algorithm and any modifications you made. Comment on the hyperparameters and why you chose them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "number_of_runs = 30000  # takes about 5 seconds\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "exploration = 0.1\n",
    "report_interval = 500\n",
    "report = 'Average: %.2f, 100-run average: %.2f (Run %d)'\n",
    "q_table = np.zeros((env.observation_space.n, env.action_space.n)) # stores learned values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process: \n",
    "Describe the training process, including any pre-processing steps such as frame stacking or converting frames to grayscale. Take short (<10 sec) videos at suitable training steps to demonstrate the agent's progress. Provide commentary on the agent's performance and any notable observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "observations, actions = env.observation_space, env.action_space\n",
    "rewards = []\n",
    "\n",
    "for run in range(number_of_runs):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    run_reward = 0\n",
    "    while not done:\n",
    "        if np.random.rand() < exploration:\n",
    "            action = actions.sample()  # Take random actions\n",
    "        else:\n",
    "            action = np.argmax(q_table[observation, :])  # Take learned action \n",
    "\n",
    "        new_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        q_table[observation, action] = (1 - learning_rate) * q_table[observation, action] + learning_rate * \\\n",
    "            (reward + discount_factor * np.max(q_table[new_observation, :]))\n",
    "        \n",
    "        run_reward += reward\n",
    "        \n",
    "        observation = new_observation\n",
    "\n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "            rewards.append(run_reward)\n",
    "            if ((run + 1) % 1000 == 0):\n",
    "                print(report % (np.mean(rewards), np.mean(rewards[-100:]), run + 1))\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Performance Metrics: \n",
    "Evaluate the performance of your trained model. Provide relevant metrics such as average reward, episodes needed to solve the game, and any additional visualizations or graphs. Comment on the strengths and limitations of your trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gymnasium.make('CartPole-v1')\n",
    "\n",
    "# Define the neural network model\n",
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(24, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(24, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(num_actions)\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Initialize the DQN model\n",
    "num_actions = env.action_space.n\n",
    "model = DQN(num_actions)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Define the epsilon-greedy exploration strategy\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "# Replay memory parameters\n",
    "memory = []\n",
    "memory_capacity = 10000\n",
    "batch_size = 32\n",
    "\n",
    "# Training parameters\n",
    "gamma = 0.99  # Discount factor\n",
    "num_episodes = 1000\n",
    "\n",
    "# Function to preprocess observations\n",
    "def preprocess_observation(obs):\n",
    "    return np.reshape(obs, [1, -1])\n",
    "\n",
    "# Training the DQN\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = preprocess_observation(state)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon-greedy exploration\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            q_values = model.predict(state)\n",
    "            action = np.argmax(q_values)  # Exploit\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = preprocess_observation(next_state)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        if len(memory) > memory_capacity:\n",
    "            del memory[0]\n",
    "        \n",
    "        # Experience replay\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = np.array(random.sample(memory, batch_size))\n",
    "            states, actions, rewards, next_states, dones = np.split(batch, 5, axis=1)\n",
    "\n",
    "            states = np.vstack(states)\n",
    "            next_states = np.vstack(next_states)\n",
    "\n",
    "            q_values = model.predict(states)\n",
    "            next_q_values = model.predict(next_states)\n",
    "\n",
    "            max_next_q_values = np.max(next_q_values, axis=1)\n",
    "            targets = rewards.squeeze() + (1 - dones.squeeze()) * gamma * max_next_q_values\n",
    "\n",
    "            q_values[range(batch_size.astype(int)), actions.squeeze().astype(int)] = targets\n",
    "            model.train_on_batch(states, q_values)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    # Display progress\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "# After training, use the model to play the game\n",
    "state = env.reset()\n",
    "state = preprocess_observation(state)\n",
    "done = False\n",
    "while not done:\n",
    "    q_values = model.predict(state)\n",
    "    action = np.argmax(q_values)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    next_state = preprocess_observation(next_state)\n",
    "    state = next_state\n",
    "    env.render()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation and Report: \n",
    "Provide a clear and detailed report of your process, including decisions, challenges, and any improvements made during the training. Include commentary on the weights chosen and any pre-processing techniques applied."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

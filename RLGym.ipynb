{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment 3: RL Gym\n",
    "### Game Selection: CartPole\n",
    "For this assignment I have chosen the 2D training tool CartPole due to its straighforward mechanics and clear reward structure. The AI is rewarded every time it takes a step and is still alive, so can be trained to improve the time it stays alive for. https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "\n",
    "\n",
    "Using some code from https://www.sliceofexperiments.com/p/an-actually-runnable-march-2023-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#Pre-setup installs\n",
    "%pip install gymnasium[classic-control]\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup/imports\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import gymnasium\n",
    "\n",
    "env = gymnasium.make(\"CartPole-v1\")  # create the environment used for the game"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation: \n",
    "Implement and train an RL model using an algorithm like Q-learning, Deep Q-Networks (DQN), or any other suitable method. Explain your choice of algorithm and any modifications you made. Comment on the hyperparameters and why you chose them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "number_of_runs = 1000\n",
    "learning_rate = 0.15\n",
    "discount_factor = 0.99\n",
    "exploration = lambda run: 50. / (run + 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process: \n",
    "Describe the training process, including any pre-processing steps such as frame stacking or converting frames to grayscale. Take short (<10 sec) videos at suitable training steps to demonstrate the agent's progress. Provide commentary on the agent's performance and any notable observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "observations, actions = env.observation_space, env.action_space\n",
    "\n",
    "for run in range(1, number_of_runs + 1):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.random() < exploration:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = env.take_action(observation) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - learning_rate) * old_value + learning_rate * (reward + next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "            observation, info = env.reset()\n",
    "            # print result\n",
    "            #if run % (number_of_runs / 10) == 0:\n",
    "                #print(agent.rewards)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Performance Metrics: \n",
    "Evaluate the performance of your trained model. Provide relevant metrics such as average reward, episodes needed to solve the game, and any additional visualizations or graphs. Comment on the strengths and limitations of your trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation and Report: \n",
    "Provide a clear and detailed report of your process, including decisions, challenges, and any improvements made during the training. Include commentary on the weights chosen and any pre-processing techniques applied."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
